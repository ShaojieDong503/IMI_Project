# -*- coding: utf-8 -*-
"""SCARF.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14H2AFlm7iM2VASWJQ5b-d6A2DpUuKtey
"""

# pip install git+https://github.com/clabrugere/pytorch-scarf.git

import torch
import torch.nn as nn
from torch import Tensor
from torch.distributions.uniform import Uniform
import torch.nn.functional as F
import numpy as np
from torch.utils.data import Dataset
import random
from tqdm.auto import tqdm
import matplotlib.pyplot as plt
import pandas as pd
from sklearn import datasets
from sklearn.linear_model import LogisticRegression
from sklearn.manifold import TSNE
from sklearn.metrics import ConfusionMatrixDisplay, classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from torch.optim import Adam
from torch.utils.data import DataLoader


# Model
class MLP(torch.nn.Sequential):
    def __init__(self, input_dim: int, hidden_dim: int, num_hidden: int, dropout: float = 0.0) -> None:
        layers = []
        in_dim = input_dim
        for _ in range(num_hidden - 1):
            layers.append(nn.Linear(in_dim, hidden_dim))
            layers.append(nn.BatchNorm1d(hidden_dim))
            layers.append(nn.ReLU(inplace=True))
            layers.append(nn.Dropout(dropout))
            in_dim = hidden_dim

        layers.append(nn.Linear(in_dim, hidden_dim))

        super().__init__(*layers)


class SCARF(nn.Module):
    def __init__(
            self,
            input_dim: int,
            features_low: int,
            features_high: int,
            dim_hidden_encoder: int,
            num_hidden_encoder: int,
            dim_hidden_head: int,
            num_hidden_head: int,
            corruption_rate: float = 0.6,
            dropout: float = 0.0,
    ) -> None:
        super().__init__()

        self.encoder = MLP(input_dim, dim_hidden_encoder, num_hidden_encoder, dropout)
        self.pretraining_head = MLP(dim_hidden_encoder, dim_hidden_head, num_hidden_head, dropout)

        # uniform disstribution over marginal distributions of dataset's features
        self.marginals = Uniform(torch.Tensor(features_low), torch.Tensor(features_high))
        self.corruption_rate = corruption_rate

    def forward(self, x: Tensor) -> Tensor:
        batch_size, _ = x.size()

        # 1: create a mask of size (batch size, m) where for each sample we set the jth column to True at random,
        # such that corruption_len / m = corruption_rate 2: create a random tensor of size (batch size, m) drawn from
        # the uniform distribution defined by the min, max values of the training set 3: replace x_corrupted_ij by
        # x_random_ij where mask_ij is true
        corruption_mask = torch.rand_like(x, device=x.device) > self.corruption_rate
        x_random = self.marginals.sample(torch.Size((batch_size,))).to(x.device)
        x_corrupted = torch.where(corruption_mask, x_random, x)

        # get embeddings
        embeddings = self.pretraining_head(self.encoder(x))
        embeddings_corrupted = self.pretraining_head(self.encoder(x_corrupted))

        return embeddings, embeddings_corrupted

    @torch.inference_mode()
    def get_embeddings(self, x: Tensor) -> Tensor:
        return self.encoder(x)


# Loss
class NTXent(nn.Module):
    def __init__(self, temperature: float = 1.0) -> None:
        """NT-Xent loss for contrastive learning using cosine distance as similarity metric as used in [SimCLR](
        https://arxiv.org/abs/2002.05709). Implementation adapted from
        https://theaisummer.com/simclr/#simclr-loss-implementation

        Args:
            temperature (float, optional): scaling factor of the similarity metric. Defaults to 1.0.
        """
        super().__init__()
        self.temperature = temperature

    def forward(self, z_i: Tensor, z_j: Tensor) -> Tensor:
        """Compute NT-Xent loss using only anchor and positive batches of samples. Negative samples are the 2*(N-1)
        samples in the batch

        Args:
            z_i (torch.tensor): anchor batch of samples
            z_j (torch.tensor): positive batch of samples

        Returns:
            float: loss
        """
        batch_size = z_i.size(0)

        # compute similarity between the sample's embedding and its corrupted view
        z = torch.cat([z_i, z_j], dim=0)
        similarity = F.cosine_similarity(z.unsqueeze(1), z.unsqueeze(0), dim=2)

        sim_ij = torch.diag(similarity, batch_size)
        sim_ji = torch.diag(similarity, -batch_size)
        positives = torch.cat([sim_ij, sim_ji], dim=0)

        mask = (~torch.eye(batch_size * 2, batch_size * 2, dtype=torch.bool, device=z_i.device)).float()
        numerator = torch.exp(positives / self.temperature)
        denominator = mask * torch.exp(similarity / self.temperature)

        all_losses = -torch.log(numerator / torch.sum(denominator, dim=1))
        loss = torch.sum(all_losses) / (2 * batch_size)

        return loss


# Dateset
class SCARFDataset(Dataset):
    def __init__(self, data, target, columns=None):
        self.data = np.array(data)
        self.target = np.array(target)
        self.columns = columns

    @property
    def features_low(self):
        return self.data.min(axis=0)

    @property
    def features_high(self):
        return self.data.max(axis=0)

    @property
    def shape(self):
        return self.data.shape

    def __getitem__(self, index):
        return torch.tensor(self.data[index], dtype=torch.float32)

    def __len__(self):
        return len(self.data)


def get_device():
    if torch.cuda.is_available():
        device = torch.device("cuda")
    elif torch.backends.mps.is_available() and torch.backends.mps.is_built():
        device = torch.device("mps")
    else:
        device = torch.device("cpu")

    return device


def train_epoch(model, criterion, train_loader, optimizer, device):
    model.train()
    epoch_loss = 0.0

    for x in train_loader:
        x = x.to(device)

        # get embeddings
        emb_anchor, emb_positive = model(x)

        # compute loss
        loss = criterion(emb_anchor, emb_positive)
        loss.backward()

        # update model weights
        optimizer.step()

        # reset gradients
        optimizer.zero_grad()

        # log progress
        epoch_loss += loss.item()

    return epoch_loss / len(train_loader.dataset)


def dataset_embeddings(model, loader, device):
    embeddings = []

    for x in tqdm(loader):
        x = x.to(device)
        embeddings.append(model.get_embeddings(x))

    embeddings = torch.cat(embeddings).cpu().numpy()

    return embeddings


def fix_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)


def task_two_script(file_path, output_path):
    df = pd.read_csv(file_path + "general_table.csv")

    data = df
    # ***Change features***
    data = data[["total_debit_amount_cad", "total_credit_amount_cad", "transaction_frequency", "funnel_points",
                 "structuring_points_y"]]

    # preprocess
    constant_cols = [c for c in data.columns if data[c].nunique() == 1]
    data = data.drop(columns=constant_cols)

    scaler = StandardScaler()
    data_scaled = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)

    # Since this package requires a target, we will create a dummy target
    dummy_target = np.zeros(len(data_scaled))
    features_low = data_scaled.min(axis=0).values
    features_high = data_scaled.max(axis=0).values
    # to torch dataset
    train_ds = SCARFDataset(data_scaled.to_numpy(),
                            target=dummy_target)
    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)

    # ***Hyperparams***
    batch_size = 256  # larger takes more time
    epochs = 1000  # as large as possible
    output_dimension = 6  # output embedding dimension
    # ***Hyperparams***

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)

    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)

    model = SCARF(
        input_dim=train_ds.shape[1],
        features_low=train_ds.features_low,
        features_high=train_ds.features_high,
        dim_hidden_encoder=output_dimension,
        # Change layout below
        num_hidden_encoder=3,
        dim_hidden_head=64,
        num_hidden_head=2,
        corruption_rate=0.5,
        dropout=0.1,
    ).to(device)

    optimizer = Adam(model.parameters(), lr=1e-4)  # learning rate, wirely, smaller lr is more fluctuated
    ntxent_loss = NTXent()  # loss function

    loss_history = []
    best_loss = float('inf')
    patience = 20
    no_improve = 0

    for epoch in range(1, epochs + 1):
        epoch_loss = train_epoch(model, ntxent_loss, train_loader, optimizer, device)
        loss_history.append(epoch_loss)

        # 早停判断
        if epoch_loss < best_loss:
            best_loss = epoch_loss
            no_improve = 0
        else:
            no_improve += 1

        if no_improve >= patience:
            print(f"\nEarly stopping at epoch {epoch}")
            break

        if epoch % 10 == 0:
            print(f"Epoch {epoch}/{epochs} - Loss: {loss_history[-1]:.4f}")

    # 可视化训练过程
    plt.figure(figsize=(10, 5))
    plt.plot(loss_history)
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.title("Training Loss Curve")
    plt.savefig(output_path + "Training Loss Curve")

    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=False)
    train_embeddings = dataset_embeddings(model, train_loader, device)

    customer_ids = df['customer_id'].values  # 或 df.index 如果 customer_id 是索引

    embedding_columns = [f'embedding_{i}' for i in range(train_embeddings.shape[1])]

    df_embeddings = pd.DataFrame(
        data=np.column_stack([customer_ids, train_embeddings]),
        columns=['customer_id'] + embedding_columns
    )

    df_embeddings['customer_id'] = df_embeddings['customer_id'].astype(df['customer_id'].dtype)

    df_embeddings.to_csv(output_path + "Task_2.csv", index=False)

    # df_embeddings.info
