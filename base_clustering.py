# -*- coding: utf-8 -*-
"""base_clustering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16UEWW5d9VKYJMusa-wcRwIIYZBSzymdW
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
from sklearn.cluster import KMeans
import plotly.express as px


# general_file_path = '/Users/christopherzhang/Desktop/2025_IMI/new_general_table.csv'
def task_one_script(general_file_path, output_path):
    df = pd.read_csv(general_file_path)

    # All the avialiable feature that could be use for cluster model
    feature_all = ['total_debit_amount_cad', 'total_credit_amount_cad',
                   'debit_count', 'credit_count', 'transaction_frequency',
                   'avg_transaction_interval_day', 'mode_transaction_interval_day',
                   'mode_transaction_type', 'date_range', 'max_credit_transaction_amount',
                   'avg_credit_transaction_amount', 'max_debit_transaction_amount',
                   'avg_debit_transaction_amount', 'structuring_points_x', 'funnel_index',
                   'funnel_points', 'structuring_points_y', 'score_missing_kyc']

    # Check the for all the feature above before building the model

    correlation_matrix = df[feature_all].corr()

    plt.figure(figsize=(10, 8))
    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', square=True)

    plt.title('Correlation Matrix Heatmap', fontsize=16)
    plt.savefig(output_path + "Correlation Matrix Heatmap")

    # This is all the final features we are going to use for clustering Removed: Total credit amount,debit_count,
    # credit_count,max_credit_transaction_amount,max_debit_transaction_amount,structuring_points_x,count_index,
    # funnel_points 'mode_transaction_type', 'date_range',

    feature_final = ['total_debit_amount_cad',
                     'transaction_frequency',
                     'avg_transaction_interval_day', 'mode_transaction_interval_day',
                     'avg_credit_transaction_amount',
                     'avg_debit_transaction_amount',
                     'funnel_points', 'structuring_points_y', 'score_missing_kyc']

    # Scale the features
    scaler = StandardScaler()
    df_final = df[feature_final]
    data_scaled = scaler.fit_transform(df_final)
    df_scaled = pd.DataFrame(data_scaled, columns=feature_final)

    # Build a KMeans Model

    k_range = range(2, 10)

    inertia = []
    silhouette_scores = []

    for k in k_range:
        kmeans = KMeans(n_clusters=k, random_state=42)
        clusters = kmeans.fit_predict(df_scaled)
        inertia.append(kmeans.inertia_)
        silhouette_scores.append(silhouette_score(df_scaled, clusters))

    plt.figure(figsize=(12, 6))
    plt.subplot(1, 2, 1)
    plt.plot(k_range, inertia, marker='o', linestyle='--', color='blue')
    plt.xticks(k_range)
    plt.xlabel('Number of Clusters (k)')
    plt.ylabel('Inertia')
    plt.title('Elbow Method for Optimal Clusters')

    plt.subplot(1, 2, 2)
    plt.plot(k_range, silhouette_scores, marker='o', linestyle='--', color='green')
    plt.xticks(k_range)
    plt.xlabel('Number of Clusters (k)')
    plt.ylabel('Silhouette Score')
    plt.title('Silhouette Method for Optimal Clusters')

    plt.tight_layout()
    plt.savefig(output_path + "Elbow Method for Optimal Clusters & Silhouette Method for Optimal Clusters")

    """The rate of decrease in inertia slows significantly after k=3"""

    # Use the Best K for the model
    chosen_k = k_range[np.argmax(silhouette_scores)]
    print(chosen_k)

    kmeans = KMeans(n_clusters=chosen_k, random_state=71)
    clusters = kmeans.fit_predict(df_scaled)
    df['cluster'] = clusters

    # Create an interactive scatter plot
    fig = px.scatter(
        df,
        x='cluster',
        y='avg_debit_transaction_amount',
        color='cluster',  # Color points by their cluster,
        title='Interactive K-Means Clustering Visualization',
    )

    # Show the interactive plot
    fig.savefig(output_path + 'Interactive K-Means Clustering Visualization')

    # Create a stacked bar plot
    fig = px.histogram(
        df,
        x='cluster',
        y='funnel_points',
        color='cluster'  # Color bars by their cluster
    )

    # Show the plot
    fig.savefig(output_path + "clusters")

    """以下是输出最后的算分和输出output file 1

    --------------------------------------------------------------------------------------------------------------------------
    """

    # Calculate the one with high transaction frequency
    def assign_high_freq_score(
            df_hhigfeq: pd.DataFrame,
            cluster_col: str = "cluster",
            freq_col: str = "transaction_frequency",
            score_col: str = "score_high_freq"
    ) -> pd.DataFrame:
        freq_95 = df_hhigfeq.groupby(cluster_col)[freq_col].transform(lambda x: x.quantile(0.95))
        freq_99 = df_hhigfeq.groupby(cluster_col)[freq_col].transform(lambda x: x.quantile(0.99))

        # Initialize or reset the score column to 0
        df_hhigfeq[score_col] = 0

        # Assign 2 points for > 99th percentile
        mask_99 = df_hhigfeq[freq_col] > freq_99
        df_hhigfeq.loc[mask_99, score_col] = 1

        # Assign 1 point for > 95th percentile (but <= 99th)
        mask_95 = (df_hhigfeq[freq_col] > freq_95) & (~mask_99)
        df_hhigfeq.loc[mask_95, score_col] = 0.5

        return df_hhigfeq

    df = assign_high_freq_score(
        df,
        cluster_col="cluster",
        freq_col="transaction_frequency",
        score_col="score_high_freq"
    )

    # Calculate the one with high transaction debit amount
    def assign_high_amount_score_debit(
            df_highamount: pd.DataFrame,
            cluster_col: str = "cluster",
            amount_col: str = "total_debit_amount_cad",
            score_col: str = "score_debit_high_amount"
    ) -> pd.DataFrame:
        amount_95 = df_highamount.groupby(cluster_col)[amount_col].transform(lambda x: x.quantile(0.95))
        amount_99 = df_highamount.groupby(cluster_col)[amount_col].transform(lambda x: x.quantile(0.99))

        # Initialize or reset the score column to 0
        df_highamount[score_col] = 0

        # Assign 2 points for > 99th percentile
        mask_99 = df_highamount[amount_col] > amount_99
        df_highamount.loc[mask_99, score_col] = 1

        # Assign 1 point for > 95th percentile (but <= 99th)
        mask_95 = (df_highamount[amount_col] > amount_95) & (~mask_99)
        df_highamount.loc[mask_95, score_col] = 0.5

        return df_highamount

    df = assign_high_amount_score_debit(
        df,
        cluster_col="cluster",
        amount_col="total_debit_amount_cad",
        score_col="score_debit_high_amount"
    )

    # Calculate the one with high transaction credit amount
    def assign_high_amount_score_credit(
            df_highamount: pd.DataFrame,
            cluster_col: str = "cluster",
            amount_col: str = "total_credit_amount_cad",
            score_col: str = "score_credit_high_amount"
    ) -> pd.DataFrame:
        amount_95 = df_highamount.groupby(cluster_col)[amount_col].transform(lambda x: x.quantile(0.95))
        amount_99 = df_highamount.groupby(cluster_col)[amount_col].transform(lambda x: x.quantile(0.99))

        # Initialize or reset the score column to 0
        df_highamount[score_col] = 0

        # Assign 2 points for > 99th percentile
        mask_99 = df_highamount[amount_col] > amount_99
        df_highamount.loc[mask_99, score_col] = 1

        # Assign 1 point for > 95th percentile (but <= 99th)
        mask_95 = (df_highamount[amount_col] > amount_95) & (~mask_99)
        df_highamount.loc[mask_95, score_col] = 0.5

        return df_highamount

    df = assign_high_amount_score_credit(
        df,
        cluster_col="cluster",
        amount_col="total_credit_amount_cad",
        score_col="score_credit_high_amount"
    )

    df["score_total"] = df["funnel_points"] + df["structuring_points_y"] + df["score_missing_kyc"] + df[
        "score_high_freq"] + df["score_debit_high_amount"] + df["score_credit_high_amount"]

    # Persons with highest score(>99%) are flaged as bad actors
    threshold = df["score_total"].quantile(0.995)
    df["bad_actor"] = df["score_total"] > threshold.astype(int)
    df.to_csv(output_path + 'Task_1.csv', index=False)

    # df['bad_actor'].value_counts()

    # bad_actors_df = df[df['bad_actor'] == 1]
    # print(bad_actors_df)
